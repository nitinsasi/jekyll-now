---
layout: post
title:  Google's free Artificial Intelligence tool will help companies identify child sexual abuse material
categories:  tech
---
 
 Using the internet as a means to spread content that sexually exploits children is one of the worst abuses imaginable. That’s why since the early 2000s Google have been investing in technology, teams, and working closely with expert organizations, like the [Internet Watch Foundation](https://www.iwf.org.uk/), to fight the spread of child sexual abuse material (CSAM) online.
 
 
Stamping out the spread of child sexual abuse material (CSAM) is a priority for big internet companies. But it’s also a difficult and harrowing job for those on the frontline — human moderators who have to identify and remove abusive content. That’s why Google is today releasing [free AI software](https://www.blog.google/around-the-globe/google-europe/using-ai-help-organizations-detect-and-report-child-sexual-abuse-material-online/) designed to help these individuals.

Most tech solutions in this domain work by checking images and videos against a catalog of previously identified abusive material. (See, for example- PhotoDNA, a tool developed by Microsoft and deployed by companies like Facebook and Twitter.) This sort of software, known as a “crawler,” is an effective way to stop people sharing known previously-identified CSAM. But it can’t catch material that hasn’t already been marked as illegal. For that, human moderators have to step in and review content themselves.

This is where Google’s new AI tool will help. Using the company’s expertise in machine vision, it assists moderators by sorting flagged images and videos and “prioritizing the most likely CSAM content for review.” This should allow for a much quicker reviewing process. In one trial, says Google, the AI tool helped a moderator “take action on 700 percent more CSAM content over the same time period.”
 